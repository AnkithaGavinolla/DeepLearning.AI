This Repository consists of Assignments from DeepLearning.AI Specialization
Following are descriptions of Assignments per course

Course1: Neural Networks and Deep Learning
 * Week2:
    * W2A2: Implement  logistic regression  with numpy library and train on Cat Vs Not Cat classification task.
  * Week3:
    * W3A1: Implement one layer neural network forward and back propagation with numpy library and train on Planar dataset (flower).
  * Week4:
    * W4A1: Implement deep neural network forward and back propagation with numpy.
    * W4A2: Train implemented deep neural network on Cat Vs Non Cat classification.


Course2: Improving Deep Neural Networks: Hyperparameter Tuning, Regularization and Optimization
  * Week1: 
    * W1A1: Implemented and experimented Zero, Random, He initialization methods on a 3 layer Neural Network.
    * W1A2: Implemented and experimented L2 Regression, DropOut Regularization on a 3 layer NN trained on football dataset.
    * W1A3: Implemented Forward and back propagation for a neural network. Implemented gradient checking and verified back propagation with gradient checking..
  * Week2:
     * W2A1: Implemented and experimented mini batch with gradient descent, mini batch gradient descent with momentum, mini batch gradient descent with Adam for a custom moon dataset trained with 3 layer NN.
           Implemented and experimented with learning rate decay  (Decay every iteration vs fixed Interval Scheduling) for each gradient descent mentioned  above.
  * Week3:
    * W3A1: Implemented 3 layered NN with TensorFlow and trained on sign language  dataset


Course4:Convolution Neural Networks
  * Week1:
    * W1A1: CNN network step by step forward propagation  implementation with numpy library.
    * W1A2: Developed a CNN network with Keras Sequential API to determine whether people in the images are smiling or not. Network is trained on the Happy House dataset and achieved an accuracy of 94%.
    * W1A3: Developed a CNN network with Keras Functional API to recognize digits from sign language images. Network is trained on SIGNS dataset and achieved an accuracy of 80%. 
  * Week2:
    * W2A1: Built and trained ResNet50 Network Model to recognize digits from sign language images. Network is trained on SIGNS dataset and achieved an accuracy of 96%. 
    * W2A2: Built an Alpaca/Non Alpaca classifier by transfer learning from a MobileNetV2 network trained on ImageNet.
  * Week3:
    * W3A1: Built a Driving Application Object Detection System to detect 80 classes with 5 anchor boxes from a pre-trained YOLOV2 network output and applying filtering and NMS.
    * W3A2: Developed a own U-Net architecture to implement semantic image segmentation for pixel wise class predictions for images. The Network is trained on CARLA self driving car dataset. 
  * Week4:
    * W4A1: Build a face verification and face recognition application on trained FaceNet model
    * W4A2: Generate new Art with NeuralStyle transfer from a content image and styling image built with VGG-19 Network.
    
    
Course5: Sequence Models
  * Week1:
    * W1A1: Implemented RNN, LSTM network forward propagation with Numpy.
    * W1A2: Built a character-level text generation model using an RNN to generate a list of dinosaur names.
    * W1A3: Built a Jazz music generation model using LSTM to generate its own Jazz music.
  * Week2:
    * W2A1: Built word analogy task using pre trained Glove Word embeddings.
    * W2A2: Built an emojifier to represent sentences with emojis. Model is trained with a 2-layer LSTM sequence classifier using word2Vec Glove embeddings.
  * Week3:
    * W3A1: Built a bi-directional LSTM network with attention for neural machine translation task to translate human-readable dates into standard ISO date format.
    * W3A2: Built a trigger word detection system with CONV1D, GRU’s to detect the word ‘activate’ in audio clips. Synthesized training/testing data set by combining raw audio clips of activates (positive word), negative words and background datasets using pydub.
  * Week4:
    * W4A1: Build an encoder and decoder transformer network with  positional encoding, self-attention, multi-head attention, Neural Network using Tensor flow.
