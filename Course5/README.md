Course5: Sequence Models
  * Week1:
    * W1A1: Implemented RNN, LSTM network forward propagation with Numpy.
    * W1A2: Built a character-level text generation model using an RNN to generate a list of dinosaur names.
    * W1A3: Built a Jazz music generation model using LSTM to generate its own Jazz music.
  * Week2:
    * W2A1: Built word analogy task using pre trained Glove Word embeddings.
    * W2A2: Built an emojifier to represent sentences with emojis. Model is trained with a 2-layer LSTM sequence classifier using word2Vec Glove embeddings.
  * Week3:
    * W3A1: Built a bi-directional LSTM network with attention for neural machine translation task to translate human-readable dates into standard ISO date format.
    * W3A2: Built a trigger word detection system with CONV1D, GRU’s to detect the word ‘activate’ in audio clips. Synthesized training/testing data set by combining raw audio clips of activates (positive word), negative words and background datasets using pydub.
  * Week4:
    * W4A1: Build an encoder and decoder transformer network with  positional encoding, self-attention, multi-head attention, Neural Network using Tensor flow.
