Course2: Improving Deep Neural Networks: Hyperparameter Tuning, Regularization and Optimization
  * Week1: 
    * W1A1: Implemented and experimented Zero, Random, He initialization methods on a 3 layer Neural Network.
    * W1A2: Implemented and experimented L2 Regression, DropOut Regularization on a 3 layer NN trained on football dataset.
    * W1A3: Implemented Forward and back propagation for a neural network. Implemented gradient checking and verified back propagation with gradient checking..
  * Week2:
     * W2A1: Implemented and experimented mini batch with gradient descent, mini batch gradient descent with momentum, mini batch gradient descent with Adam for a custom moon dataset trained with 3 layer NN.
           Implemented and experimented with learning rate decay  (Decay every iteration vs fixed Interval Scheduling) for each gradient descent mentioned  above.
  * Week3:
    * W3A1: Implemented 3 layered NN with TensorFlow and trained on sign language  dataset
